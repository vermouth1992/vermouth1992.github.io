[{"authors":null,"categories":null,"content":"I am a PhD candidate in Computer Science at University of Southern California (USC). Currently, I am a research assistant working at Data Science Lab led by Professor Viktor Prasanna. Before joining USC, I received my bachlor degree from Chien-Shiung Wu College at Southeast University.\nMy research focus is applied machine learning and data science, especially reinforcement learning on energy systems. Besides, I am interested in parallel computing of deep learning algorithms on heterogeneous architectures.\n","date":1546300800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1546300800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a PhD candidate in Computer Science at University of Southern California (USC). Currently, I am a research assistant working at Data Science Lab led by Professor Viktor Prasanna. Before joining USC, I received my bachlor degree from Chien-Shiung Wu College at Southeast University.","tags":null,"title":"Chi Zhang","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Wowchemy\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://vermouth1992.github.io/talk/example-talk.html","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk.html","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":[],"categories":[],"content":"Overview    Overview Algorithm  The main loop of model-based reinforcement learning is shown above. It consists:\n Collecting data using current policy and add it to the dataset. Train a world model using the current dataset. Train policy using world model.  The main challenge of the model-based RL is:\n The imperfections of the trained world model is unavoidable. Thus, compounding errors when training policy using world models need to be addressed. The design of the world model is challenging and the stochasticity is crucial for a good world model.  World Models   Architecture of the the stochastic model with discrete latent.  Stochasticity Fitting a density model is impractical for large observation space. Thus, the author considers STOCHASTIC VARIATIONAL VIDEO PREDICTION (SV2P) proposed in Babaeizadeh et al. Given $N$ transitions, our goal is to maximize the $\\prod_{t=1}^{N} p_{\\theta}(x_{t+1}|x_t, a_t)$. We apply variational inference by adding time-variant latent variable $z_t$. We use an inference network $q_{\\phi}(z_t|x_{t+1},x_{t},a_{t})$ to approximate the true posterior $p(z_t|x_{t+1},x_{t},a_{t})$ and outputs $\\mathcal{N}(\\mu_{\\phi}, \\sigma_{\\phi})$. This network is trained using reparameterization trick as: $$ z_t=\\mu_{\\phi}+\\sigma_{\\phi}\\times \\epsilon_t,\\quad \\epsilon_t\\sim \\mathcal{N}(0, I) $$ Here, $\\theta$ and $\\phi$ are the parameters of the generative model and inference network, respectively. To learn these parameters, we optimize the variational lower bound as: $$ \\begin{equation} \\mathcal{L}(x)=-\\mathbb{E}_{q_{\\phi}}[\\log p_{\\theta}(x_{t+1}|x_t, a_t, z_t)] + D_{KL}(q_{\\phi}, p(z_t)) \\end{equation} $$ where $D_{KL}$ is the Kullback-Leibler divergence between the approximated posterior and assumed prior $p(z)$ which in our case is the standard Gaussian $\\mathcal{N}(0, I)$.\nThe training of the model is divided into three phases:\n The inference network is turned off and only the generative model is trained. The inference network is used without KL divergence loss. The KL divergence loss is added.  The author argues that the weights of KL divergence are game-dependent and that\u0026rsquo;s unacceptable if we want to apply to a broad portfolio of Atari games without hyperparameter tuning. Second, the weight is usually a small number within $[e^{-5}, e^{-3}]$ which means that the approximated posterior can diverge significantly from the assumed prior. The author addresses these problems by using discrete latent variable into bits (zeros and ones) similar to Kaiser \u0026amp; Bengio (2018). The authors also train an auxilliary LSTM-based recurrent network to predict these bits autoregressively. At inference time, the latent bits will be generated by this auxiliary network in contrast to sampling from a prior. To make the predictive model more robust to unseen latent bits, we add uniform noise to approximated latent values before discretization and apply dropout Srivastava et al., 2014 on bits after discretization.\nLoss function They tried both categorical softmax and $L_2$ loss and apply clipped loss as $\\max(Loss, C)$ for constant $C$. In categorical loss, $C$ is 10. In $L_2$ loss, $C$ is 0.03. That means, if the level of confidence about the correct pixel value exceeds 97% (as $-\\ln(0.97) \\approx 0.03$) we get no gradients from that pixel any longer.\nScheduled sampling They mitigate the problem of compounding errors in simulated environment by randomly replacing in training some frames of input $X$ by the prediction from the previous steps. Typically, we linearly increase the mixing probability during training arriving at 100% around the middle of the first iteration of the training loop.\nPolicy Training When training policy in simulated environment, the problem is still the compounding errors with long trajectories. To mitigate the problem, they choose short rollouts and uniformly sample the starting state from the ground-truth buffer. However, short rollouts may have a degrading effect as the PPO algorithm does not have a way to infer effects longer than the rollout length. To ease this problem, in the last step of a rollout we add to the reward the evaluation of the value function that is learned along the policy as baselines to compute the advantages.\nResults   Sample efficiency compared to Rainbow.  Each bar illustrates the number of interactions with environment required by Rainbow to achieve the same score as our method (SimPLe). The red line indicates the 100K interactions threshold used by our method. Please refer to the paper for more results.\nReference Model Based Reinforcement Learning for Atari\n","date":1562284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567893932,"objectID":"47292a1e3d8c106d82d88dbeee81eee2","permalink":"https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari.html","publishdate":"2019-07-05T00:00:00Z","relpermalink":"/post/model-based-reinforcement-learning-for-atari.html","section":"post","summary":"Model-free reinforcement learning requires extensive interaction with the environment in order to achieve very good performance in Atari games. This will cause problems in situations where large amount of trial-and-errors are impossible or time-consuming. In this paper, the author presents Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and achieve comparable performance with model-free RL approach on various of games in only 100K interactions between the agent and the environments.","tags":[],"title":"Model Based Reinforcement Learning for Atari (Paper Summary)","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://vermouth1992.github.io/slides/example.html","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example.html","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Chi Zhang","Sanmukh R. Kuppannagari","Chuanxiu Xiong","Rajgopal Kannan","Viktor Prasanna"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8cc64233f2b50db10c0095e231917301","permalink":"https://vermouth1992.github.io/publication/iotdi-2019.html","publishdate":"2019-09-07T22:27:40.221754Z","relpermalink":"/publication/iotdi-2019.html","section":"publication","summary":"Internet-of-Things (IoT) enabled monitoring and control capabilities are enabling increasing numbers of household users with controllable loads to actively participate in smart grid energy management. Realizing an efficient real-time energy management system that takes advantage of these developments requires novel techniques for managing the increased complexity of the control action space in resolving multiple challenges such as the uncertainty in energy prices and renewable energy output along with the need to satisy physical grid constraints such as transformer capacity. Addressing these challenges, we develop a multi-household energy management framework for residential units connected to the same transformer and containing DERs such as PV, ESS and controllable loads. The goal of our framework is to schedule controllable household appliances and ESS such that the cost of procuring electricity from the utility over a horizon is minimized while physical grid constraints are satisfied at each scheduling step. Traditional energy management frameworks either perform global optimization to satisfy grid constraints but suffer from high computational complexity (for example Integer Program, Mixed Integer Programming frameworks and centralized reinforcement learning) or perform decentralized real-time energy management without satisfying global grid constraints (for example multi-agent reinforcement learning with no cooperation). In contrast, we propose a cooperative multi-agent reinforcement learning (MARL) framework that i) operates in real-time, and ii) performs explicit collaboration to satisfy global grid constraints. The novelty in our framework is two fold. Firstly, our framework trains multiple independent learners (IL) for each household in parallel using historical data and performs real-time inferencing of control actions using the most recent system state. Secondly, our framework contains a low complexity knapsack based cooperation agent which combines the outputs of ILs to minimize cost while satisfying grid constraints. Simulation results show that our cooperative MARL approach achieves significant cost improvement over centralized reinforcement learning and day-ahead planning baselines. Moreover, our approach strictly satisfies physical constraints with no apriori knowledge of system dynamics while the baseline approaches have occasional violations. We also measure the training and inference time by ranging the number of households from 1 to 25. Results show that our cooperative MARL approach scales best among various approaches.","tags":["deep reinforcement learning","internet-of-things","multi-agent","real-time load scheduling","smart home"],"title":"A Cooperative Multi-agent Deep Reinforcement Learning Framework for Real-time Residential Load Scheduling","type":"publication"},{"authors":["Chi Zhang","Corey Chen","Limian Zhang"],"categories":[],"content":"Problem Formulation In this project, we would like to manage portfolio by distributing our investment into a number of stocks based on the market. We define our environment similar to this paper Jiang et al.\nConcretely, we define $N$ to be the number of stocks we would like to invest. Without losing generality, at initial timestamp, we assume our total investment volume is 1 dollar. We define close/open relative price vector as: $$ \\begin{equation} y_t=[1, \\frac{v_{1,t, close}}{v_{1,t, open}}, \\frac{v_{2,t, close}}{v_{2,t, open}},\\cdots,\\frac{v_{N,t,close}}{v_{N,t, open}}] \\end{equation} $$ where $\\frac{v_{i, t, close}}{v_{i, t, open}}$ is the relative price of stock $i$ at timestamp $t$. Note $y[0]$ represents the relative price of cash, which is always 1. We define portfolio weight vector as: $$ \\begin{equation} w_t=[w_{0,t}, w_{1, t}, \\cdots, w_{N, t}] \\end{equation} $$ where $w_{i,t}$ represents the fraction of investment on stock $i$ at timestamp $t$ and $\\sum_{i=0}^{N}w_{i, t}=1$. Note that $w_{0,t}$ represents the fraction of cash that we maintain. Then the profit after timestamp $T$ is: $$ \\begin{equation} \\label{profit} p_T=\\prod_{t=1}^{T}y_t\\cdot w_{t-1} \\end{equation} $$ where $w_0=[1, 0, \\cdots, 0]$. If we consider a trading cost factor $\\mu$, then the trading cost of each timestamp is: $$ \\begin{equation} {\\mu}_t=\\mu\\sum{|\\frac{y_t \\odot w_{t-1}}{y_t \\cdot w_{t-1}} - w_{t}|} \\end{equation} $$ where $\\odot$ is element-wise product. Then equation \\ref{profit} becomes: $$ \\begin{equation} \\label{profit_mu} p_T=\\prod_{t=1}^{T}(1-\\mu_t) y_t\\cdot w_{t-1} \\end{equation} $$\nKey Assumptions and Goal To model real world market trades, we make several assumptions to simplify the problems:\n We can get any information about the stocks before timestamp $t$ for stock $i$. e.g. The previous stock price, the news and tweets online. Our investment will not change how the market behaves. The way we calculate profit in equation \\ref{profit} can be interpreted as: At timestamp $t$, we buy stocks according to the portfolio weight vector $w_{t-1}$ computed by history data at **open** price and sell all the stocks at **close** price. This may not be true in practice because you will not always be able to **buy/sell** the stock at **open/close** price.  The goal of portfolio management is to maximum $p_T$ by choosing portfolio weight vector $w$ at each timestamp $t$ based on history stock information.\nMDP formulation State and Action We define state $s_t$ as $o_t$, where $o_t$ is the obseration of timestamp $t$. As the time goes by, the impact of history data decreases. Thus, we only consider the history price and news in a fixed window length $W$. Hence, $$ o_t=[\\vec{v_{1,t}},\\vec{v_{2,t}},\\cdots,\\vec{v_{N,t}}] $$ where $\\vec{v_{i,t}} =\\begin{bmatrix}v_{i,t-W} \\\\ v_{i,t-W+1} \\\\ \\vdots \\\\ v_{i,t-1}\\end{bmatrix}$ and $N$ is the number of stocks. The action $a_{t}$ is just portfolio weight vector $w_{t}$. Note that this is a continuous state and action space problem. We try to directly solve it in continuous space instead of using discretization in previous work Du et al and Jin et al. Essentially, we want to train a policy network $\\pi_\\theta(a_t|o_t)$.\nState Transition The underlining state evolution is determined by the market, which we don\u0026rsquo;t have any control. What we can get is the observation state, which is the price. Since we will collect history price of various stocks, $o_t$ is given by the dataset instead of $o_{t-1}$.\nReward Instead of having reward 0 at each timestamp and $p_T$ at the end, we take logarithm of equation \\ref{profit_mu}: $$\\log{p_T}=\\log{\\prod_{t=1}^{T}\\mu_t y_t\\cdot w_{t-1}}=\\sum_{t=1}^{T}\\log(\\mu_t y_t\\cdot w_{t-1})$$ Thus, we have $\\log(\\mu_t y_t\\cdot w_{t-1})$ reward each timestamp, which avoids the sparsity of reward problem.\nDatasets Stocks used: We choose 16 target stocks from NASDAQ100 that we feel are representative of different sectors in the index fund. They include \u0026ldquo;AAPL\u0026rdquo;, \u0026ldquo;ATVI\u0026rdquo;, \u0026ldquo;CMCSA\u0026rdquo;, \u0026ldquo;COST\u0026rdquo;, \u0026ldquo;CSX\u0026rdquo;, \u0026ldquo;DISH\u0026rdquo;, \u0026ldquo;EA\u0026rdquo;, \u0026ldquo;EBAY\u0026rdquo;, \u0026ldquo;FB\u0026rdquo;, \u0026ldquo;GOOGL\u0026rdquo;, \u0026ldquo;HAS\u0026rdquo;, \u0026ldquo;ILMN\u0026rdquo;, \u0026ldquo;INTC\u0026rdquo;, \u0026ldquo;MAR\u0026rdquo;, \u0026ldquo;REGN\u0026rdquo; and \u0026ldquo;SBUX\u0026rdquo;.\nPrice Data: We collected history price of the stocks from 2012-08-13 to 2017-08-11. The price on each day contains open, high, low and close. We use 2012-08-13 to 2015-08-12 as training data and 2015-08-13 to 2017-08-11 as testing data.\nNews Data: We gather all tweets referencing the stocks from 2016-03-28 to 2016-06-15.\nAdditional Testing Data: As another form of backtesting, we randomly select 16 stocks from NASDAQ100 the network has never seen and test whether the network can generalize well.\nMethods We mainly consider model-free approach in our project. First, we train a predictor given a fixed history window length $W$ of price and news. With the predicted price, we can train a policy network $\\pi_{\\theta}(a_t|s_t)$ to maximize our portofolio value at the end of the trading period. Note that in practice, they are trained end-to-end instead of separately.\nData Preprocessing Numerical Data Instead of using the raw open, high, low and close price, we normalize the history price as $(\\frac{close}{open} - 1)\\times scale$. There are two main advantages:\n The history price are all in the same scale regardless of their actual price. Since the final portfolio is determined by the close/open ratio of each timestamp, it serves as a better feature compared with raw price.  The scale factor is heuristically set to 100 in our experiments. For missing data, we pad all open, high, low, close as the close price of previous day.\nNews Data The steps to proprocess the news data are:\n First, we filter each tweets by the most frequenct 2000 words. We pad each word with end of sentence mark to make them equally long. We use a sequence of end of sentence mark to fill the missing days.  Predictor Network    CNN predictor based on numerical history  The rationale behind the CNN predictor is adpated from Jiang et al. For each timeseries of each stock, we use a $1\\times3$ kernel to gather information in each window, then combine all the information to produce a single vector for each stock. Note that these convolutional windows doesn\u0026rsquo;t cover information across any two stocks.   LSTM predictor based on numerical history  The LSTM predictor based on numerical history is very straight forward. Note that all the stocks share the same LSTM.   LSTM predictor based on history of news  For news based approach, we try to predict the $\\frac{close-open}{open}$ ratio. We use pretrained 50d-GloVe to initialize the embedding layer followed by a LSTM and a fully-connected layer.\nPolicy Network    Policy Network  All the policy network follows the same topology with different predictors.\nOptimal Action and Imitation Learning Suppose we know the stock price of tomorrow, we greedily choose the stock with the highest close/open ratio (taking into account trading cost of changing stocks), buying as much as possible on the open and selling all at the close and it will yield the maximum portfolio value.\nGiven this fact, we can collect ground truth labels for each timestamp by choosing the optimal action. (e.g. $[0, 0, 1, \\cdots, 0, 0]$, the stock with highest close/open to be 1 and all the others to be 0) Then, we can train a policy network by imitating optimal action conditioned on the current observation of history prices $o_t$.\nDeep Deterministic Policy Gradient (DDPG)    Deep Deterministic Policy Gradient Algorithm Lillicrap et al  We try to directly learn a policy network $\\pi_{\\theta}(a_t|s_t)$ by continuous action space reinforcement learning algorithm Lillicrap et al. We show the algorithm above. The configurations are:\n Actor Network: The same as policy network. Critic Network: A linear combination of actor network structure of state (observation) and action. Exploration noise: Ornstein-Uhlenbeck with zero mean, 0.3 sigma and 0.15 theta. For fairness, we train models with different settings in 500 episodes. Some of the models are not fully converged at that time though.  Results Imitation Learning    Results of trading on testing data using policy trained by imitation learning  The market value is obtained by equally distributing your investment to all the stocks. It turns out that smaller windows work better. Larger window means larger models and it tends to overfit very quickly since the training data is just around 1000.\nDDPG    Results of trading on testing data using policy trained by DDPG  We find that LSTM predictor based models have better performance than the CNN predictor based models. It is not surprising that smaller windows perform best because of the temporaral relationship among price in a short window.\nImitation Learning vs DDPG Generally, models trained by DDPG is better than models trained by imitation learning. The difficulty of training good policy networks by imitation learning lies in the extreme tendency to overfit while DDPG doesn\u0026rsquo;t suffer from this problem because each episode during training is sampled from the whole training period with different starting date and lengths.\nTesting on Unseen Stocks    Results of trading on 16 unseen stocks using best two policies trained by imitation learning and DDPG  To see how the network generalizes, we choose another 16 unseen stocks including \u0026lsquo;FOX\u0026rsquo;, \u0026lsquo;FISV\u0026rsquo;, \u0026lsquo;EXPE\u0026rsquo;, \u0026lsquo;FAST\u0026rsquo;, \u0026lsquo;ESRX\u0026rsquo;, \u0026lsquo;DLTR\u0026rsquo;, \u0026lsquo;CTSH\u0026rsquo;, \u0026lsquo;CSCO\u0026rsquo;, \u0026lsquo;QCOM\u0026rsquo;, \u0026lsquo;PCLN\u0026rsquo;, \u0026lsquo;CELG\u0026rsquo;, \u0026lsquo;AMGN\u0026rsquo;, \u0026lsquo;WFM\u0026rsquo;, \u0026lsquo;WDC\u0026rsquo;, \u0026lsquo;NVDA\u0026rsquo; and \u0026lsquo;STX\u0026rsquo;. The result shows that these models generalizes pretty well and CNN-based predictor outperforms LSTM-based predictor in both cases.\nRelated Work In Du et al and Jin et al, the author proposes to use DQN to trade in 2 stocks market with discreted state and action space. Their settings are far simpler than ours. In Jiang et al, the author proposes to use Deterministic Policy Gradient (DPG) to trade in bitcoin market. It\u0026rsquo;s very hard to compare with them because of different dataset. But DDPG is strictly better than DPG in terms of performance and convergence time.\nFuture Work  Combine news based predictor with numerical base predictor and train on a larger dataset with finer timestamp end-to-end with policy network. Model ensemble: combine the result of models trained by DDPG and imitation learning. Online learning: automatic scrapper news and data each day, update the network and make the action. Instead of optimizing portfolio value, we consider risk aware portfolio by optimizing the sharpe ratio define as $\\frac{\\text{Mean}(r_t)}{\\text{Variance}(r_t)}$, where $r_t$ is the rate of return in the period. It turns out to be a very difficult problem since it is not a standard MDP anymore.  Pretrained Network Our code containing pretrained network is open source on Github.\n","date":1511382376,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567886524,"objectID":"52ed35e581ea4eb632eddb0513a9d91e","permalink":"https://vermouth1992.github.io/post/cs599.html","publishdate":"2017-11-22T12:26:16-08:00","relpermalink":"/post/cs599.html","section":"post","summary":"This is a course project done in Fall 2017 CSCI 599 Deep Learning and its Applications at USC.","tags":["Deep Learning","Reinforcement Learning","Portfolio Management","Convolutional Neural Networks","Long Short-Term Memory"],"title":"Deep Reinforcement Learning for Portfolio Management","type":"post"},{"authors":["Chi Zhang","Viktor Prasanna"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"7cbf240cb649e1f086bce27d43a2595e","permalink":"https://vermouth1992.github.io/publication/fpga-2017.html","publishdate":"2019-09-07T22:27:40.226734Z","relpermalink":"/publication/fpga-2017.html","section":"publication","summary":"We present a novel mechanism to accelerate state-of-art Convolutional Neural Networks (CNNs) on CPU-FPGA platform with coherent shared memory. First, we exploit Fast Fourier Transform (FFT) and Overlap-and-Add (OaA) to reduce the computational requirements of the convolutional layer. We map the frequency domain algorithms onto a highly-parallel OaA-based 2D convolver design on the FPGA. Then, we propose a novel data layout in shared memory for efficient data communication between the CPU and the FPGA. To reduce the memory access latency and sustain peak performance of the FPGA, our design employs double buffering. To reduce the inter-layer data remapping latency, we exploit concurrent processing on the CPU and the FPGA. Our approach can be applied to any kernel size less than the chosen FFT size with appropriate zero-padding leading to acceleration of a wide range of CNN models. We exploit the data parallelism of OaA-based 2D convolver and task parallelism to scale the overall system performance.\nBy using OaA, the number of floating point operations is reduced by 39.14% to 54.10% for the state-of-art CNNs. We implement VGG16, AlexNet and GoogLeNet on Intel QuickAssist QPI FPGA Platform. These designs sustain 123.48 GFLOPs/sec, 83.00 GFLOPs/sec and 96.60 GFLOPs/sec, respectively. Compared with the state-of-the-art AlexNet implementation, our design achieves 1.35x GFLOPs/sec improvement using 3.33x less multipliers and 1.1x less memory. Compared with the state-of-art VGG16 implementation, our design has 0.66x GFLOPs/sec using 3.48x less multipliers without impacting the classification accuracy. For GoogLeNet implementation, our design achieves 5.56x improvement in performance compared with 16 threads running on a 10 Core Intel Xeon Processor at 2.8 GHz. ","tags":["CPU","FPGA","concurrent processing","convolutional neural networks","discrete fourier transform","double buffering","overlap-and-add","shared memory"],"title":"Frequency Domain Acceleration of Convolutional Neural Networks on CPU-FPGA Shared Memory System","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://vermouth1992.github.io/admin.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]
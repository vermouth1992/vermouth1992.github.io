<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.4.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Chi Zhang">

  
  
  
    
  
  <meta name="description" content="Model-free reinforcement learning requires extensive interaction with the environment in order to achieve very good performance in Atari games. This will cause problems in situations where large amount of trial-and-errors are impossible or time-consuming. In this paper, the author presents Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and achieve comparable performance with model-free RL approach on various of games in only 100K interactions between the agent and the environments.">

  
  <link rel="alternate" hreflang="en-us" href="https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.0/css/all.css" integrity="sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

  

  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400|Roboto:400,400italic,700|Roboto+Mono|Open Sans:300,600&display=swap">
  

  
  
  
  <link rel="stylesheet" href="/css/academic.min.aaf227612f94f962b625a029029c6170.css">

  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-147277683-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="https://www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/">

  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Chi Zhang">
  <meta property="og:url" content="https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/">
  <meta property="og:title" content="Model Based Reinforcement Learning for Atari (Paper Summary) | Chi Zhang">
  <meta property="og:description" content="Model-free reinforcement learning requires extensive interaction with the environment in order to achieve very good performance in Atari games. This will cause problems in situations where large amount of trial-and-errors are impossible or time-consuming. In this paper, the author presents Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and achieve comparable performance with model-free RL approach on various of games in only 100K interactions between the agent and the environments."><meta property="og:image" content="https://vermouth1992.github.io/img/icon-192.png">
  <meta property="twitter:image" content="https://vermouth1992.github.io/img/icon-192.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2019-07-05T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2019-09-07T15:05:32-07:00">
  

  

    






  





  





  


<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/"
    },
    "headline": "Model Based Reinforcement Learning for Atari (Paper Summary)",
    
    "datePublished": "2019-07-05T00:00:00Z",
    "dateModified": "2019-09-07T15:05:32-07:00",
    
    "author": {
      "@type": "Person",
      "name": "Chi Zhang"
    },
    
    "description": "Model-free reinforcement learning requires extensive interaction with the environment in order to achieve very good performance in Atari games. This will cause problems in situations where large amount of trial-and-errors are impossible or time-consuming. In this paper, the author presents Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and achieve comparable performance with model-free RL approach on various of games in only 100K interactions between the agent and the environments."
  }
</script>

  


  


  





  <title>Model Based Reinforcement Learning for Atari (Paper Summary) | Chi Zhang</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0 compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Chi Zhang</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav mr-auto">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#experience"><span>Experience</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/ChiZhang_CV.pdf"><span>CV</span></a>
        </li>

        
        

      
      </ul>
      <ul class="navbar-nav ml-auto">
      

        

        
        <li class="nav-item">
          <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        

        
        <li class="nav-item">
          <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
        </li>
        

      </ul>

    </div>
  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Model Based Reinforcement Learning for Atari (Paper Summary)</h1>

  

  
    



<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Sep 7, 2019
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/model-based-reinforcement-learning-for-atari/#disqus_thread"></a>
  

  
  

  
    
<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/&amp;text=Model%20Based%20Reinforcement%20Learning%20for%20Atari%20%28Paper%20Summary%29" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/&amp;t=Model%20Based%20Reinforcement%20Learning%20for%20Atari%20%28Paper%20Summary%29" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Model%20Based%20Reinforcement%20Learning%20for%20Atari%20%28Paper%20Summary%29&amp;body=https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/&amp;title=Model%20Based%20Reinforcement%20Learning%20for%20Atari%20%28Paper%20Summary%29" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Model%20Based%20Reinforcement%20Learning%20for%20Atari%20%28Paper%20Summary%29%20https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://vermouth1992.github.io/post/model-based-reinforcement-learning-for-atari/&amp;title=Model%20Based%20Reinforcement%20Learning%20for%20Atari%20%28Paper%20Summary%29" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>


  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      

<h2 id="overview">Overview</h2>

<p>





<figure>

  <a data-fancybox="" href="main_loop.png" >

<img src="main_loop.png" >
</a>


<figcaption data-pre="Figure " data-post=":" >
  <h4>Overview Algorithm</h4>
  
</figcaption>

</figure>

The main loop of model-based reinforcement learning is shown above. It consists:</p>

<ol>
<li>Collecting data using current policy and add it to the dataset.</li>
<li>Train a world model using the current dataset.</li>
<li>Train policy using world model.</li>
</ol>

<p>The main challenge of the model-based RL is:</p>

<ul>
<li>The imperfections of the trained world model is unavoidable. Thus, compounding errors when training policy using world models need to be addressed.</li>
<li>The design of the world model is challenging and the <code>stochasticity</code> is crucial for a good world model.</li>
</ul>

<h2 id="world-models">World Models</h2>







<figure>

  <a data-fancybox="" href="model_architecture.png" >

<img src="model_architecture.png" >
</a>


<figcaption data-pre="Figure " data-post=":" >
  <h4>Architecture of the the stochastic model with discrete latent.</h4>
  
</figcaption>

</figure>


<h3 id="stochasticity">Stochasticity</h3>

<p>Fitting a density model is impractical for large observation space. Thus, the author considers STOCHASTIC VARIATIONAL VIDEO PREDICTION (SV2P) proposed in <a href="https://arxiv.org/abs/1710.11252" target="_blank">Babaeizadeh et al</a>. Given $N$ transitions, our goal is to maximize the $\prod_{t=1}^{N} p_{\theta}(x_{t+1}|x_t, a_t)$. We apply variational inference by adding time-variant latent variable $z_t$. We use an inference network $q_{\phi}(z_t|x_{t+1},x_{t},a_{t})$ to approximate the true posterior $p(z_t|x_{t+1},x_{t},a_{t})$ and outputs $\mathcal{N}(\mu_{\phi}, \sigma_{\phi})$. This network is trained using reparameterization trick as:
$$
z_t=\mu_{\phi}+\sigma_{\phi}\times \epsilon_t,\quad \epsilon_t\sim \mathcal{N}(0, I)
$$
Here, $\theta$ and $\phi$ are the parameters of the generative model and inference network, respectively. To learn these parameters, we optimize the variational lower bound as:
$$
\begin{equation}
\mathcal{L}(x)=-\mathbb{E}_{q_{\phi}}[\log p_{\theta}(x_{t+1}|x_t, a_t, z_t)] + D_{KL}(q_{\phi}, p(z_t))
\end{equation}
$$
where $D_{KL}$ is the Kullback-Leibler divergence between the approximated posterior and assumed
prior $p(z)$ which in our case is the standard Gaussian $\mathcal{N}(0, I)$.</p>

<p>The training of the model is divided into three phases:</p>

<ol>
<li>The inference network is turned off and only the generative model is trained.</li>
<li>The inference network is used without KL divergence loss.</li>
<li>The KL divergence loss is added.</li>
</ol>

<p>The author argues that the weights of KL divergence are game-dependent and that&rsquo;s unacceptable if we want to apply to a broad portfolio of Atari games without hyperparameter tuning. Second, the weight is usually a small number within $[e^{-5}, e^{-3}]$ which means that the approximated posterior can diverge significantly from the assumed prior. The author addresses these problems by using discrete latent variable into bits (zeros and ones) similar to <a href="https://arxiv.org/abs/1801.09797" target="_blank">Kaiser &amp; Bengio (2018)</a>. The authors also train an auxilliary LSTM-based recurrent network to predict these bits autoregressively. At inference time, the latent bits will be generated by this auxiliary network in contrast to sampling from a prior. To make the predictive model more robust to unseen latent bits, we add uniform noise to approximated latent values before discretization and apply dropout <a href="https://dl.acm.org/citation.cfm?id=2670313" target="_blank">Srivastava et al., 2014</a> on bits after discretization.</p>

<h3 id="loss-function">Loss function</h3>

<p>They tried both categorical softmax and $L_2$ loss and apply <code>clipped loss</code> as $\max(Loss, C)$ for constant $C$. In categorical loss, $C$ is 10. In $L_2$ loss, $C$ is 0.03. That means, if the level of confidence about the correct pixel value exceeds 97% (as $-\ln(0.97) \approx 0.03$) we get no gradients from that pixel any longer.</p>

<h3 id="scheduled-sampling">Scheduled sampling</h3>

<p>They mitigate the problem of compounding errors in simulated environment by randomly replacing in training some frames of input $X$ by the prediction from the previous steps. Typically, we linearly increase the mixing probability during training arriving at 100% around the middle of the first iteration of the training loop.</p>

<h2 id="policy-training">Policy Training</h2>

<p>When training policy in simulated environment, the problem is still the compounding errors with long trajectories. To mitigate the problem, they choose short rollouts and uniformly sample the starting state from the ground-truth buffer. However, short rollouts may have a degrading effect as the PPO algorithm does not have a way to infer effects longer than the rollout length. To ease this problem, in the last step of a rollout we add to the reward the evaluation of the value function that is learned along the policy as baselines to compute the advantages.</p>

<h2 id="results">Results</h2>







<figure>

  <a data-fancybox="" href="result_rainbow.png" >

<img src="result_rainbow.png" >
</a>


<figcaption data-pre="Figure " data-post=":" >
  <h4>Sample efficiency compared to Rainbow.</h4>
  
</figcaption>

</figure>


<p>Each bar illustrates the number of interactions with environment required by Rainbow to achieve the same score as our method (SimPLe). The red line indicates the 100K interactions threshold used by our method.
Please refer to the paper for more results.</p>

<h2 id="reference">Reference</h2>

<p><a href="https://arxiv.org/abs/1903.00374" target="_blank">Model Based Reinforcement Learning for Atari</a></p>

    </div>

    


    



    
      








  





  
  
  
    
  
  
  <div class="media author-card" itemscope itemtype="http://schema.org/Person">
    
      
      <img class="portrait mr-3" src="/authors/admin/avatar_huc58505316b018906e49d12f391640d0e_1881019_250x250_fill_q90_lanczos_center.jpg" itemprop="image" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title" itemprop="name"><a href="https://vermouth1992.github.io/">Chi Zhang</a></h5>
      <h6 class="card-subtitle">PhD in Computer Science</h6>
      
      <ul class="network-icon" aria-hidden="true">
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://www.linkedin.com/in/chi-zhang-00979082/" target="_blank" rel="noopener">
              <i class="fab fa-linkedin"></i>
            </a>
          </li>
        
          
          
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://scholar.google.com/citations?hl=en&amp;user=z83Itm8AAAAJ" target="_blank" rel="noopener">
              <i class="ai ai-google-scholar"></i>
            </a>
          </li>
        
          
          
          
            
          
          
          
          
          
            
          
          <li>
            <a itemprop="sameAs" href="https://github.com/vermouth1992" target="_blank" rel="noopener">
              <i class="fab fa-github"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>



      
      
    

    

    
<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "https-vermouth1992-github-io" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js" integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/languages/r.min.js"></script>
        
      

      
      
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>
      
    

    
    

    
    
    <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script>
      const search_index_filename = "/index.json";
      const i18n = {
        'placeholder': "Search...",
        'results': "results found",
        'no_results': "No results found"
      };
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://https-vermouth1992-github-io.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.16bbb3750feb7244c9bc409a5a4fe678.js"></script>

    






  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    Copyright &copy; 2019 Chi Zhang. All rights reserved. &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
